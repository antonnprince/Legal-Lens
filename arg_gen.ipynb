!pip install unsloth
!pip install -U "huggingface_hub[cli]"

%%shell
huggingface-cli login --token YOUR_TOKEN


import torch
from datasets import load_dataset
from trl import SFTTrainer



from unsloth import FastLanguageModel
model,tokenizer = FastLanguageModel.from_pretrained(
    model_name='unsloth/Llama-3.2-3B-Instruct-bnb-4bit',
     max_seq_length=2048,  #maximum number of tokens the model can process in a single input sequence
    load_in_4bit=True,
    dtype=None
)

model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules = ["q_proj","o_proj","k_proj","gate_proj","up_proj","down_proj"],
    lora_alpha = 16,
 lora_dropout = 0, # Optimized at 0
 bias = "none", # No additional bias terms
 use_gradient_checkpointing = "unsloth", # Gradient checkpointing to save memory
 random_state = 3407,
 use_rslora = False
)

dataset = load_dataset('opennyaiorg/aalap_instruction_dataset',split="train")
import pandas as pd
dataset=pd.DataFrame(dataset)
dataset.columns

dataset = dataset[dataset["task"].str.contains("argument_generation")]
dataset.head()

dataset["input_text"] = dataset["input_text"].str.replace(r'[\n"""[\]/]', '', regex=True)
dataset.head()
# dataset.head()

from unsloth.chat_templates import standardize_sharegpt

# dataset = dataset.copy()

# # dataset.head()


from unsloth.chat_templates import get_chat_template,standardize_sharegpt

# ds.drop_column("conversations")

ds = dataset.copy()

def format_prompts(row):
    return[
            {"content": row['system_prompt'], "role": "system"},
            {"content": row['input_text'], "role": "user"},
            {"content": row['output_text'], "role": "assistant"}
        ]
    
# Apply the corrected format
ds["conversations"] = ds.apply(format_prompts, axis=1)

ds["conversations"]

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.2",
)

ds["text"] = ds["conversations"].apply(
    lambda convo: tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)
)

ds.head()
